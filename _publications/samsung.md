---
title: "PawFACS: Leveraging Semi-Supervised Learning for Pet Facial Action Recognition"
collection: publications
permalink: /publication/samsung
excerpt: 'My work as an intern at Samsung Research where I developed a multi-label classification system for detecting facial action units in dogs and cats using a semi-supervised pseudo-labeling strategy, significantly enhancing the understanding of pet emotions by annotating 8,918 images and showing that the Swin Transformer-Tiny model achieves the highest accuracy on the CatVsDog dataset.'
date: 2024-11-25
venue: 'Proceedings of the 35th British Machine Vision Conference (BMVC) 2024, Glasgow, UK'
image: 'wavemixsr.jpg'
width: '800'
---
Nowadays, Pets are regarded as integral family members because they provide emo- tional support and bonding, yet understanding and assessing their emotions can be chal- lenging. Pet facial action detection, which helps determine their emotional states, re- mains a significant and unsolved research issue. The lack of labeled data, however, presents a challenge, as manually creating labels for various species and breeds is labor- intensive. Drawing from DogFACS and CatFACS standards, we present a multi-label classification system for pet facial action units (AUs) in dogs and cats. Our approach centers on two main objectives: (i) a semi-supervised pseudo-labeling strategy to address the lack of labeled pet facial AU data, and (ii) providing annotations for 8,918 images with multiple pet facial action labels to assist future research in this field. We introduce a semi-supervised learning framework that utilizes a small portion of precisely labeled data, gradually extracting reliable samples from the unlabeled data pool. Various deep neural models based on transformers and convolutional neural networks are evaluated on the CatVsDog benchmark dataset. Our comparison study reveals that SwinTransformer- Tiny outperformed other models with an accuracy of 88.93 % on the CatVsDog dataset. Our effort attempts to bridge the gap between pet and human facial action understanding.


Work in Samsung R&D Institute Banglore as an Intern

[Paper Link](https://bmvc2024.org/proceedings/288/)

